---
title: "**k-Nearest Neighbors algorithm (k-NN) in the Iris data set**"
author: Xavier Vivancos García
date: '`r Sys.Date()`'
output: 
  html_document:
    number_sections: yes
    toc: yes
    theme: cosmo
    highlight: tango
---

# **Introduction**

We are going to work with the well-known supervised machine learning algorithm called k-NN or [k-Nearest Neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). 
For this exercise, we will use the [Iris data set](https://archive.ics.uci.edu/ml/datasets/iris) for classification. The attribute `Species` of the data set will be the variable
that we want to predict.

# **Loading data** {.tabset .tabset-fade .tabset-pills}

First we need to load some libraries.

```{r message=FALSE, warning=FALSE}
# Load libraries
library(knitr)
library(class)
library(tidyverse)
library(GGally)
```

Let's get an idea of what we're working with.

## First rows 
```{r message=FALSE, warning=FALSE}
# First rows 
head(iris)
```

## Last rows 
```{r message=FALSE, warning=FALSE}
# Last rows 
tail(iris)
```

## Summary 
```{r message=FALSE, warning=FALSE}
# Summary
summary(iris)
```

## Structure
```{r message=FALSE, warning=FALSE}
# Structure 
str(iris)
```

# **Data analysis**

```{r fig.align='center', message=FALSE, warning=FALSE}
# Some minor changes in the data set
iris2 <- iris %>%
  rename(`Sepal length`=Sepal.Length,
         `Sepal width`=Sepal.Width,
         `Petal length`=Petal.Length,
         `Petal width`=Petal.Width) %>%
  mutate(Species=fct_recode(Species, "Setosa"="setosa",
                            "Versicolor"="versicolor",
                            "Virginica"="virginica"))

# Histogram for each species
iris2 %>%
  gather(Attributes, Value, 1:4) %>%
  ggplot(aes(x=Value, fill=Attributes)) +
  geom_histogram(colour="black") +
  facet_wrap(~Species) +
  theme_bw() +
  labs(x="Values", y="Frequency",
       title="Iris data set",
       subtitle="Histogram for each species") +
  theme(legend.title=element_blank(),
        legend.position="bottom")
```

```{r fig.align='center', message=FALSE, warning=FALSE}
# Density plot for each species
iris2 %>%
  gather(Attributes, value, 1:4) %>%
  ggplot(aes(x=value, fill=Species)) +
  geom_density(colour="black", alpha=0.5) +
  facet_wrap(~Attributes, scales="free_x") +
  labs(x="Values", y="Density",
       title="Iris data set",
       subtitle="Density plot for each attribute") +
  theme_bw() +
  theme(legend.position="bottom",
        legend.title=element_blank())
```

```{r fig.align='center', message=FALSE, warning=FALSE}
# Violin plot for each attribute
iris2 %>%
  gather(Attributes, value, 1:4) %>%
  ggplot(aes(x=reorder(Attributes, value, FUN=median), y=value, fill=Attributes)) +
  geom_violin(show.legend=FALSE) +
  geom_boxplot(width=0.05, fill="white") +
  labs(title="Iris data set",
       subtitle="Violin plot for each attribute") +
  theme_bw() +
  theme(axis.title.y=element_blank(),
        axis.title.x=element_blank())
```

```{r fig.align='center', message=FALSE, warning=FALSE}
# Boxplot for each attribute
iris2 %>%
  gather(Attributes, value, 1:4) %>%
  ggplot(aes(x=reorder(Attributes, value, FUN=median), y=value, fill=Attributes)) +
  geom_boxplot(show.legend=FALSE) +
  labs(title="Iris data set",
       subtitle="Boxplot for each attribute") +
  theme_bw() +
  theme(axis.title.y=element_blank(),
        axis.title.x=element_blank())
```

```{r fig.align='center', message=FALSE, warning=FALSE}
# Scatter plot and correlations
ggpairs(cbind(iris2, Cluster=as.factor(iris2$Species)),
        columns=1:4, aes(colour=Cluster, alpha=0.5),
        lower=list(continuous="points"),
        axisLabels="none", switch="both") +
        theme_bw() 
```

# **Data preparation** 

We have to normalize the quantitative variables to express them in the same range of values. 

```{r message=FALSE, warning=FALSE}
# Normalization of all columns except Species
dataNorm <- iris
dataNorm[, -5] <- scale(iris[, -5])
```

Then we split the data set into two parts: a training set and a test set. The first is used to train the system, while the second is used to evaluate the trained system.

```{r message=FALSE, warning=FALSE}
# Reproducible results
set.seed(1234)

# 70% train and 30% test
ind <- sample(2, nrow(dataNorm), replace=TRUE, prob=c(0.7, 0.3))
trainData <- dataNorm[ind==1,]
testData <- dataNorm[ind==2,]
```

Once we have done the data analysis and the data set has been normalized and divided in two parts, we can execute the k-NN algorithm. 

# **k-NN execution**

The [`knn()`](https://www.rdocumentation.org/packages/class/versions/7.3-15/topics/knn) function has the following main arguments:

* `train`. Matrix or data frame of training set cases.

* `test`. Matrix or data frame of test set cases. A vector will be interpreted as a row vector for a single case.

* `cl`. Factor of true classifications of training set.

* `k`. Number of neighbours considered.

```{r message=FALSE, warning=FALSE}
# Execution of k-NN with k=1
KnnTestPrediction_k1 <- knn(trainData[,-5], testData[,-5],
                            trainData$Species, k=1, prob=TRUE)

# Execution of k-NN with k=2
KnnTestPrediction_k2 <- knn(trainData[,-5], testData[,-5],
                            trainData$Species, k=2, prob=TRUE)

# Execution of k-NN with k=3
KnnTestPrediction_k3 <- knn(trainData[,-5], testData[,-5],
                            trainData$Species, k=3, prob=TRUE)

# Execution of k-NN with k=4
KnnTestPrediction_k4 <- knn(trainData[,-5], testData[,-5],
                            trainData$Species, k=4, prob=TRUE)
```

# **Evaluation**

We can use the confusion matrix to evaluate the accuracy of the previous classifications with different values of `k`, and study which one offers the best results. 

```{r message=FALSE, warning=FALSE}
# Confusion matrix of KnnTestPrediction_k1
table(testData$Species, KnnTestPrediction_k1)
```

How do we interpret this matrix? 

* The 10 observations in the test data corresponding to setosa species are correctly predicted as setosa. 

* The 12 observations in the test data corresponding to versicolor species are correctly predicted as versicolor. 

* 14 of the 16 observations in the test data corresponding to virginica species are correctly predicted as virginica. The other two are misclassified as versicolor. 

We can calculate the classification accuracy as follows, 

```{r message=FALSE, warning=FALSE}
# Classification accuracy of KnnTestPrediction_k1
sum(KnnTestPrediction_k1==testData$Species)/length(testData$Species)*100
```

The results of the other classifications:

```{r message=FALSE, warning=FALSE}
# Confusion matrix of KnnTestPrediction_k2
table(testData$Species, KnnTestPrediction_k2)

# Classification accuracy of KnnTestPrediction_k2
sum(KnnTestPrediction_k2==testData$Species)/length(testData$Species)*100

# Confusion matrix of KnnTestPrediction_k3
table(testData$Species, KnnTestPrediction_k3)

# Classification accuracy of KnnTestPrediction_k3
sum(KnnTestPrediction_k3==testData$Species)/length(testData$Species)*100

# Confusion matrix of KnnTestPrediction_k4
table(testData$Species, KnnTestPrediction_k4)

# Classification accuracy of KnnTestPrediction_k4
sum(KnnTestPrediction_k4==testData$Species)/length(testData$Species)*100
```

To study graphically which value of `k` gives us the best classification, we can plot Accuracy vs Choice of `k`. 

```{r fig.align='center', message=FALSE, warning=FALSE}
# Empty variables
KnnTestPrediction <- list()
accuracy <- numeric()

# From k=1 to k=100...
for(k in 1:100){

  # KnnTestPrediction for each k
  KnnTestPrediction[[k]] <- knn(trainData[,-5], testData[,-5], trainData$Species, k, prob=TRUE)
    
  # Accuracy for each k   
  accuracy[k] <- sum(KnnTestPrediction[[k]]==testData$Species)/length(testData$Species)*100

}

# Accuracy vs Choice of k
plot(accuracy, type="b", col="dodgerblue", cex=1, pch=20,
     xlab="k, number of neighbors", ylab="Classification accuracy", 
     main="Accuracy vs Neighbors")

# Add lines indicating k with best accuracy
abline(v=which(accuracy==max(accuracy)), col="darkorange", lwd=1.5)

# Add line for max accuracy seen
abline(h=max(accuracy), col="grey", lty=2)

# Add line for min accuracy seen 
abline(h=min(accuracy), col="grey", lty=2)
```

We see that 10 different values of `k` achieve the highest accuracy. Also notice that, as `k` increases, the accuracy decreases. 

# **Summary**

In this kernel we have learned about the k-Nearest Neighbors algorithm, including the data preparation before we execute it (data normalization and division in two parts) 
and the evaluation of the results. 

# **Citations for used packages**

Dua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.

Venables, W. N. & Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth Edition. Springer, New York. ISBN 0-387-95457-0

Yihui Xie (2018). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.20.

Yihui Xie (2015) Dynamic Documents with R and knitr. 2nd edition. Chapman and Hall/CRC. ISBN 978-1498716963

Yihui Xie (2014) knitr: A Comprehensive Tool for Reproducible Research in R. In Victoria Stodden, Friedrich Leisch and Roger D. Peng, editors, Implementing Reproducible 
Computational Research. Chapman and Hall/CRC. ISBN 978-1466561595

Hadley Wickham (2017). tidyverse: Easily Install and Load the ‘Tidyverse’. R package version 1.2.1. https://CRAN.R-project.org/package=tidyverse

Barret Schloerke, Jason Crowley, Di Cook, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg and Joseph Larmarange (2018). GGally: Extension to 'ggplot2'. 
R package version 1.4.0. https://CRAN.R-project.org/package=GGally